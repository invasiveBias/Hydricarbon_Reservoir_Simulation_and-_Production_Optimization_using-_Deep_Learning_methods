{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown\n!gdown 1-9laqLN6xJU1_3QR2o31omFm8qej_BH6\n!gdown 1-ElKbjVTcxC2kaMGhYcT3aoI0GckibAl\n!gdown 1-KCH5_k-VAMzp5nJhsBcPg1lXSQcaIyY","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:03:07.867942Z","iopub.execute_input":"2025-02-22T19:03:07.868146Z","iopub.status.idle":"2025-02-22T19:03:32.517217Z","shell.execute_reply.started":"2025-02-22T19:03:07.868126Z","shell.execute_reply":"2025-02-22T19:03:32.516088Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.17.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2025.1.31)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1-9laqLN6xJU1_3QR2o31omFm8qej_BH6\nFrom (redirected): https://drive.google.com/uc?id=1-9laqLN6xJU1_3QR2o31omFm8qej_BH6&confirm=t&uuid=78225f7b-64bc-4514-9de8-7ef14a8d905a\nTo: /kaggle/working/complete_structured_train_data.csv\n100%|████████████████████████████████████████| 281M/281M [00:03<00:00, 76.4MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-ElKbjVTcxC2kaMGhYcT3aoI0GckibAl\nTo: /kaggle/working/complete_structured_test_data.csv\n100%|███████████████████████████████████████| 98.0M/98.0M [00:00<00:00, 204MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1-KCH5_k-VAMzp5nJhsBcPg1lXSQcaIyY\nTo: /kaggle/working/complete_structured_eval_data.csv\n100%|███████████████████████████████████████| 42.0M/42.0M [00:00<00:00, 115MB/s]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nstr_train_data = pd.read_csv('complete_structured_train_data.csv')\nstr_test_data = pd.read_csv('complete_structured_test_data.csv')\nstr_eval_data = pd.read_csv('complete_structured_eval_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:03:32.518273Z","iopub.execute_input":"2025-02-22T19:03:32.518627Z","iopub.status.idle":"2025-02-22T19:03:37.598486Z","shell.execute_reply.started":"2025-02-22T19:03:32.518602Z","shell.execute_reply":"2025-02-22T19:03:37.597708Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# turning the 'drill_type' column into numerical values\nstr_train_data['drill_type'] = str_train_data['drill_type'].apply(lambda x: 1 if x == 'producer' else 2)\nstr_test_data['drill_type'] = str_test_data['drill_type'].apply(lambda x: 1 if x == 'producer' else 2)\nstr_eval_data['drill_type'] = str_eval_data['drill_type'].apply(lambda x: 1 if x == 'producer' else 2)\nstr_train_data['z'] = str_train_data[['k_lower','k_upper']].apply(lambda x: x[0] - x[1],axis=1)\nstr_test_data['z'] = str_test_data[['k_lower','k_upper']].apply(lambda x: x[0] - x[1],axis=1)\nstr_eval_data['z'] = str_eval_data[['k_lower','k_upper']].apply(lambda x: x[0] - x[1],axis=1)\nstr_train_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:03:37.599272Z","iopub.execute_input":"2025-02-22T19:03:37.599553Z","iopub.status.idle":"2025-02-22T19:04:38.901469Z","shell.execute_reply.started":"2025-02-22T19:03:37.599533Z","shell.execute_reply":"2025-02-22T19:04:38.900626Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-3-166132ae090c>:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  str_train_data['z'] = str_train_data[['k_lower','k_upper']].apply(lambda x: x[0] - x[1],axis=1)\n<ipython-input-3-166132ae090c>:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  str_test_data['z'] = str_test_data[['k_lower','k_upper']].apply(lambda x: x[0] - x[1],axis=1)\n<ipython-input-3-166132ae090c>:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  str_eval_data['z'] = str_eval_data[['k_lower','k_upper']].apply(lambda x: x[0] - x[1],axis=1)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   simulation_number  counter  drill_type  assumed_bottom_hole_pressure  \\\n0                  0        0           2                    690.417276   \n1                  0        0           1                    613.611011   \n2                  0        1           2                    990.657604   \n3                  0        1           1                    908.651369   \n4                  0        2           2                    893.137126   \n\n   full_gas_pr  full_oil_pr  full_water_ir  full_water_pr     h_perm  k_lower  \\\n0     0.000000     0.000000     690.417297       0.000000   73.76108     15.0   \n1   852.919006   613.611023     690.417297       1.249074  271.43484     15.0   \n2   852.919312   613.611023    1681.074829       0.936913   79.36990     15.0   \n3  2115.447266  1522.262329    1681.074829       1.060416  178.44434     15.0   \n4  4180.928711  3016.384277    2574.211914       1.375655  108.25170     15.0   \n\n   k_upper  porosity    v_perm     x     y     z  \n0      1.0     0.087  0.737611  23.0  10.0  14.0  \n1      1.0     0.087  2.714348  13.0   4.0  14.0  \n2      1.0     0.087  0.793699  14.0  12.0  14.0  \n3      1.0     0.087  1.784443   3.0  11.0  14.0  \n4      2.0     0.097  1.082517  13.0   7.0  13.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>simulation_number</th>\n      <th>counter</th>\n      <th>drill_type</th>\n      <th>assumed_bottom_hole_pressure</th>\n      <th>full_gas_pr</th>\n      <th>full_oil_pr</th>\n      <th>full_water_ir</th>\n      <th>full_water_pr</th>\n      <th>h_perm</th>\n      <th>k_lower</th>\n      <th>k_upper</th>\n      <th>porosity</th>\n      <th>v_perm</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>690.417276</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>690.417297</td>\n      <td>0.000000</td>\n      <td>73.76108</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>0.087</td>\n      <td>0.737611</td>\n      <td>23.0</td>\n      <td>10.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>613.611011</td>\n      <td>852.919006</td>\n      <td>613.611023</td>\n      <td>690.417297</td>\n      <td>1.249074</td>\n      <td>271.43484</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>0.087</td>\n      <td>2.714348</td>\n      <td>13.0</td>\n      <td>4.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>990.657604</td>\n      <td>852.919312</td>\n      <td>613.611023</td>\n      <td>1681.074829</td>\n      <td>0.936913</td>\n      <td>79.36990</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>0.087</td>\n      <td>0.793699</td>\n      <td>14.0</td>\n      <td>12.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>908.651369</td>\n      <td>2115.447266</td>\n      <td>1522.262329</td>\n      <td>1681.074829</td>\n      <td>1.060416</td>\n      <td>178.44434</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>0.087</td>\n      <td>1.784443</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>893.137126</td>\n      <td>4180.928711</td>\n      <td>3016.384277</td>\n      <td>2574.211914</td>\n      <td>1.375655</td>\n      <td>108.25170</td>\n      <td>15.0</td>\n      <td>2.0</td>\n      <td>0.097</td>\n      <td>1.082517</td>\n      <td>13.0</td>\n      <td>7.0</td>\n      <td>13.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"str_train_data = pd.pivot_table(str_train_data,\n                             index=['simulation_number', 'counter','drill_type'],\n                             values=[ 'assumed_bottom_hole_pressure', 'x', 'y', 'z',\n                                     'h_perm','v_perm','porosity',\n                                     'full_gas_pr', 'full_oil_pr', 'full_water_ir', 'full_water_pr'])\nstr_test_data = pd.pivot_table(str_test_data,\n                             index=['simulation_number', 'counter','drill_type'],\n                             values=[ 'assumed_bottom_hole_pressure', 'x', 'y', 'z',\n                                     'h_perm','v_perm','porosity',\n                                     'full_gas_pr', 'full_oil_pr', 'full_water_ir', 'full_water_pr'])\nstr_eval_data = pd.pivot_table(str_eval_data,\n                             index=['simulation_number', 'counter','drill_type'],\n                             values=[ 'assumed_bottom_hole_pressure', 'x', 'y', 'z',\n                                     'h_perm','v_perm','porosity',\n                                     'full_gas_pr', 'full_oil_pr', 'full_water_ir', 'full_water_pr'])\nstr_train_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:38.902351Z","iopub.execute_input":"2025-02-22T19:04:38.902609Z","iopub.status.idle":"2025-02-22T19:04:41.629116Z","shell.execute_reply.started":"2025-02-22T19:04:38.902577Z","shell.execute_reply":"2025-02-22T19:04:41.628262Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                      assumed_bottom_hole_pressure  \\\nsimulation_number counter drill_type                                 \n0                 0       1                             613.611011   \n                          2                             690.417276   \n                  1       1                             908.651369   \n                          2                             990.657604   \n                  2       1                             790.195912   \n\n                                      full_gas_pr  full_oil_pr  full_water_ir  \\\nsimulation_number counter drill_type                                            \n0                 0       1            852.919006   613.611023     690.417297   \n                          2              0.000000     0.000000     690.417297   \n                  1       1           2115.447266  1522.262329    1681.074829   \n                          2            852.919312   613.611023    1681.074829   \n                  2       1           3210.543701  2312.458252    1681.074829   \n\n                                      full_water_pr     h_perm  porosity  \\\nsimulation_number counter drill_type                                       \n0                 0       1                1.249074  271.43484     0.087   \n                          2                0.000000   73.76108     0.087   \n                  1       1                1.060416  178.44434     0.087   \n                          2                0.936913   79.36990     0.087   \n                  2       1                1.186372  115.32066     0.087   \n\n                                        v_perm     x     y     z  \nsimulation_number counter drill_type                              \n0                 0       1           2.714348  13.0   4.0  14.0  \n                          2           0.737611  23.0  10.0  14.0  \n                  1       1           1.784443   3.0  11.0  14.0  \n                          2           0.793699  14.0  12.0  14.0  \n                  2       1           1.153207   6.0  16.0  14.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>assumed_bottom_hole_pressure</th>\n      <th>full_gas_pr</th>\n      <th>full_oil_pr</th>\n      <th>full_water_ir</th>\n      <th>full_water_pr</th>\n      <th>h_perm</th>\n      <th>porosity</th>\n      <th>v_perm</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n    <tr>\n      <th>simulation_number</th>\n      <th>counter</th>\n      <th>drill_type</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">0</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>1</th>\n      <td>613.611011</td>\n      <td>852.919006</td>\n      <td>613.611023</td>\n      <td>690.417297</td>\n      <td>1.249074</td>\n      <td>271.43484</td>\n      <td>0.087</td>\n      <td>2.714348</td>\n      <td>13.0</td>\n      <td>4.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>690.417276</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>690.417297</td>\n      <td>0.000000</td>\n      <td>73.76108</td>\n      <td>0.087</td>\n      <td>0.737611</td>\n      <td>23.0</td>\n      <td>10.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>1</th>\n      <td>908.651369</td>\n      <td>2115.447266</td>\n      <td>1522.262329</td>\n      <td>1681.074829</td>\n      <td>1.060416</td>\n      <td>178.44434</td>\n      <td>0.087</td>\n      <td>1.784443</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>990.657604</td>\n      <td>852.919312</td>\n      <td>613.611023</td>\n      <td>1681.074829</td>\n      <td>0.936913</td>\n      <td>79.36990</td>\n      <td>0.087</td>\n      <td>0.793699</td>\n      <td>14.0</td>\n      <td>12.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <th>1</th>\n      <td>790.195912</td>\n      <td>3210.543701</td>\n      <td>2312.458252</td>\n      <td>1681.074829</td>\n      <td>1.186372</td>\n      <td>115.32066</td>\n      <td>0.087</td>\n      <td>1.153207</td>\n      <td>6.0</td>\n      <td>16.0</td>\n      <td>14.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_tensor = str_train_data.reset_index()\ntrain_tensor.drop(['simulation_number','counter'], axis=1, inplace=True)\ntrain_tensor = train_tensor[['drill_type',\t'z','x','y','h_perm',\t'porosity',\t'v_perm'\n,'assumed_bottom_hole_pressure','full_gas_pr',\t'full_oil_pr',\t'full_water_ir',\t'full_water_pr']]\ntest_tensor = str_test_data.reset_index()\ntest_tensor.drop(['simulation_number','counter'], axis=1, inplace=True)\ntest_tensor = test_tensor[['drill_type',\t'z','x','y','h_perm',\t'porosity',\t'v_perm'\n,'assumed_bottom_hole_pressure','full_gas_pr',\t'full_oil_pr',\t'full_water_ir',\t'full_water_pr']]\neval_tensor = str_eval_data.reset_index()\neval_tensor.drop(['simulation_number','counter'], axis=1, inplace=True)\neval_tensor = eval_tensor[['drill_type',\t'z','x','y','h_perm',\t'porosity',\t'v_perm'\n,'assumed_bottom_hole_pressure','full_gas_pr',\t'full_oil_pr',\t'full_water_ir',\t'full_water_pr']]\ntrain_tensor.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:41.630038Z","iopub.execute_input":"2025-02-22T19:04:41.630314Z","iopub.status.idle":"2025-02-22T19:04:41.947523Z","shell.execute_reply.started":"2025-02-22T19:04:41.630293Z","shell.execute_reply":"2025-02-22T19:04:41.946745Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   drill_type     z     x     y     h_perm  porosity    v_perm  \\\n0           1  14.0  13.0   4.0  271.43484     0.087  2.714348   \n1           2  14.0  23.0  10.0   73.76108     0.087  0.737611   \n2           1  14.0   3.0  11.0  178.44434     0.087  1.784443   \n3           2  14.0  14.0  12.0   79.36990     0.087  0.793699   \n4           1  14.0   6.0  16.0  115.32066     0.087  1.153207   \n\n   assumed_bottom_hole_pressure  full_gas_pr  full_oil_pr  full_water_ir  \\\n0                    613.611011   852.919006   613.611023     690.417297   \n1                    690.417276     0.000000     0.000000     690.417297   \n2                    908.651369  2115.447266  1522.262329    1681.074829   \n3                    990.657604   852.919312   613.611023    1681.074829   \n4                    790.195912  3210.543701  2312.458252    1681.074829   \n\n   full_water_pr  \n0       1.249074  \n1       0.000000  \n2       1.060416  \n3       0.936913  \n4       1.186372  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>drill_type</th>\n      <th>z</th>\n      <th>x</th>\n      <th>y</th>\n      <th>h_perm</th>\n      <th>porosity</th>\n      <th>v_perm</th>\n      <th>assumed_bottom_hole_pressure</th>\n      <th>full_gas_pr</th>\n      <th>full_oil_pr</th>\n      <th>full_water_ir</th>\n      <th>full_water_pr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>14.0</td>\n      <td>13.0</td>\n      <td>4.0</td>\n      <td>271.43484</td>\n      <td>0.087</td>\n      <td>2.714348</td>\n      <td>613.611011</td>\n      <td>852.919006</td>\n      <td>613.611023</td>\n      <td>690.417297</td>\n      <td>1.249074</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>14.0</td>\n      <td>23.0</td>\n      <td>10.0</td>\n      <td>73.76108</td>\n      <td>0.087</td>\n      <td>0.737611</td>\n      <td>690.417276</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>690.417297</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>14.0</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>178.44434</td>\n      <td>0.087</td>\n      <td>1.784443</td>\n      <td>908.651369</td>\n      <td>2115.447266</td>\n      <td>1522.262329</td>\n      <td>1681.074829</td>\n      <td>1.060416</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>14.0</td>\n      <td>14.0</td>\n      <td>12.0</td>\n      <td>79.36990</td>\n      <td>0.087</td>\n      <td>0.793699</td>\n      <td>990.657604</td>\n      <td>852.919312</td>\n      <td>613.611023</td>\n      <td>1681.074829</td>\n      <td>0.936913</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>14.0</td>\n      <td>6.0</td>\n      <td>16.0</td>\n      <td>115.32066</td>\n      <td>0.087</td>\n      <td>1.153207</td>\n      <td>790.195912</td>\n      <td>3210.543701</td>\n      <td>2312.458252</td>\n      <td>1681.074829</td>\n      <td>1.186372</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"class NumericalPreprocessing:\n    def __init__(self):\n        self.mms_dict = {}\n        self.stat_values = {}  # For PercentileTransform\n\n    def min_max_scaling(self, array, tag):\n        # ... (min_max_scaling function remains the same)\n        min_vals = np.min(array, axis=0, keepdims=True)  # Min values for each column\n        max_vals = np.max(array, axis=0, keepdims=True)  # Max values for each column\n\n        # Ensure no division by zero if min and max are the same for a column\n        range_vals = max_vals - min_vals\n        range_vals = np.where(range_vals == 0, np.ones_like(range_vals), range_vals)\n\n        scaled_array = (array - min_vals) / range_vals\n        self.mms_dict[tag] = (range_vals, min_vals)\n        return scaled_array\n\n    def reverse_scaling(self, scaled_array, tag):\n        if isinstance(scaled_array, torch.Tensor):\n            scaled_array = scaled_array.numpy()\n\n        range_vals, min_vals = self.mms_dict[tag]\n        original_array = (scaled_array * range_vals) + min_vals\n        return original_array\n\n\n    def fit_distribution(self, dataset,tag):\n        mean = np.mean(dataset,axis=0)\n        std = np.std(dataset,axis=0)\n        self.stat_values[tag] = (mean, std)\n        \n    def transform_distribution(self, dataset,tag):\n        if self.stat_values is None:\n            raise RuntimeError(\"fit_distirbution method must be called before transform_percentile.\")\n\n        mean = self.stat_values[tag][0]\n        std = self.stat_values[tag][1]\n        dataset = np.where(dataset <= (mean - 3.75 * std), 2, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 3.75 * std), dataset <= (mean - 3.5 * std)), 3, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 3.5 * std), dataset <= (mean - 3.25 * std)), 4, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 3.25 * std), dataset <= (mean - 3 * std)), 5, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 3 * std), dataset <= (mean - 2.75 * std)), 6, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 2.75 * std), dataset <= (mean - 2.5 * std)), 7, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 2.5 * std), dataset <= (mean - 2.25 * std)), 8, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 2.25 * std), dataset <= (mean - 2 * std)), 9, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 2 * std), dataset <= (mean - 1.75 * std)), 10, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 1.75 * std), dataset <= (mean - 1.5 * std)), 11, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 1.5 * std), dataset <= (mean - 1.25 * std)), 12, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 1.25 * std), dataset <= (mean - 1 * std)), 13, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 1 * std), dataset <= (mean - 0.75 * std)), 14, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 0.75 * std), dataset <= (mean - 0.5 * std)), 15, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 0.5 * std), dataset <= (mean - 0.25 * std)), 16, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean - 0.25 * std), dataset <= mean ), 17, dataset)\n        dataset = np.where(np.logical_and(dataset > mean, dataset <= (mean + 0.25 * std )), 18, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 0.25 * std), dataset <= (mean + 0.5 * std )), 19, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 0.5 * std), dataset <= (mean + 0.75 * std )), 20, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 0.75 * std), dataset <= (mean + 1 * std )), 21, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 1 * std), dataset <= (mean + 1.25 * std )), 22, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 1.25 * std), dataset <= (mean + 1.5 * std )), 23, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 1.5 * std), dataset <= (mean + 1.75 * std )), 24, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 1.75 * std), dataset <= (mean + 2 * std )), 25, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 2 * std), dataset <= (mean + 2.25 * std )), 26, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 2.25 * std), dataset <= (mean + 2.5 * std )), 27, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 2.5 * std), dataset <= (mean + 2.75 * std )), 28, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 2.75 * std), dataset <= (mean + 3 * std )), 29, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 3 * std), dataset <= (mean + 3.25 * std )), 30, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 3.25 * std), dataset <= (mean + 3.5 * std )), 31, dataset)\n        dataset = np.where(np.logical_and(dataset > (mean + 3.5 * std), dataset <= (mean + 3.75 * std )), 32, dataset)\n        dataset = np.where(dataset > (mean + 3.75 * std), 33, dataset)\n        return dataset\n\n    def reverse_distribution(self, dataset,tag):\n        \n        if self.stat_values is None:\n            raise RuntimeError(\"fit_distribution method must be called before reverse_distribution.\")\n        if isinstance(dataset, torch.Tensor):\n            dataset = dataset.numpy()\n\n        mean = self.stat_values[tag][0]\n        std = self.stat_values[tag][1]\n        \n        dataset = np.where(dataset == 2 ,(mean - 3.75*std),dataset)\n        dataset = np.where(dataset == 3,(mean - 3.5 * std) ,dataset)\n        dataset = np.where(dataset == 4, (mean - 3.25 * std),dataset)\n        dataset = np.where(dataset == 5, (mean - 3 * std) ,dataset)\n        dataset = np.where(dataset == 6, (mean - 2.75 * std) ,dataset)\n        dataset = np.where(dataset == 7 , (mean - 2.5 * std) ,dataset)\n        dataset = np.where(dataset == 8, (mean - 2.25 * std) ,dataset)\n        dataset = np.where(dataset == 9, (mean - 2 * std) ,dataset)\n        dataset = np.where(dataset == 10 ,(mean - 1.75*std),dataset)\n        dataset = np.where(dataset == 11,(mean - 1.5 * std) ,dataset)\n        dataset = np.where(dataset == 12, (mean - 1.25 * std),dataset)\n        dataset = np.where(dataset == 13, (mean - 1 * std) ,dataset)\n        dataset = np.where(dataset == 14, (mean - 0.75 * std) ,dataset)\n        dataset = np.where(dataset == 15 , (mean - 0.5 * std) ,dataset)\n        dataset = np.where(dataset == 16, (mean - 0.25 * std) ,dataset)\n        dataset = np.where(dataset == 17, (mean - std) ,dataset)\n        dataset = np.where(dataset == 18, mean ,dataset)\n        dataset = np.where(dataset == 19 ,(mean + 0.25*std),dataset)\n        dataset = np.where(dataset == 20,(mean + 0.5 * std) ,dataset)\n        dataset = np.where(dataset == 21, (mean + 0.75 * std),dataset)\n        dataset = np.where(dataset == 22, (mean + 1 * std) ,dataset)\n        dataset = np.where(dataset == 23, (mean + 1.25 * std) ,dataset)\n        dataset = np.where(dataset == 24 , (mean + 1.5 * std) ,dataset)\n        dataset = np.where(dataset == 25, (mean + 1.75 * std) ,dataset)\n        dataset = np.where(dataset == 26, (mean + 2 * std) ,dataset)\n        dataset = np.where(dataset == 27 ,(mean + 2.25*std),dataset)\n        dataset = np.where(dataset == 28,(mean + 2.5 * std) ,dataset)\n        dataset = np.where(dataset == 29, (mean + 2.75 * std),dataset)\n        dataset = np.where(dataset == 30, (mean + 3 * std) ,dataset)\n        dataset = np.where(dataset == 31, (mean + 3.25 * std) ,dataset)\n        dataset = np.where(dataset == 32 , (mean + 3.5 * std) ,dataset)\n        dataset = np.where(dataset == 33, (mean  + 3.75 * std) ,dataset)\n        \n        return dataset\n\n    def fit_transform_distribution(self, array,tag):\n        self.fit_distribution(array,tag)\n        transformed_array = self.transform_distribution(array,tag)\n        return transformed_array\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:41.949309Z","iopub.execute_input":"2025-02-22T19:04:41.949557Z","iopub.status.idle":"2025-02-22T19:04:41.971754Z","shell.execute_reply.started":"2025-02-22T19:04:41.949537Z","shell.execute_reply":"2025-02-22T19:04:41.971053Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"npp = NumericalPreprocessing()\n# seperate the inputs and scaled targets\nX_train,y_train = train_tensor.values[:,:8] ,npp.min_max_scaling(train_tensor.values[:,8:],'y_train')\nX_test,y_test = test_tensor.values[:,:8], npp.min_max_scaling(test_tensor.values[:,8:],'y_test')\nX_eval,y_eval = eval_tensor.values[:,:8], npp.min_max_scaling(eval_tensor.values[:,8:],'y_eval')\n\n# transform the continous columns of the inputs\nX_train[:,4:] = npp.fit_transform_distribution(X_train[:,4:],'x_train')\nX_test[:,4:] = npp.fit_transform_distribution(X_test[:,4:],'x_test')\nX_eval[:,4:] = npp.fit_transform_distribution(X_eval[:,4:],'x_eval')\n\n\nx_train_view = pd.DataFrame(X_train)\nx_train_view.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:41.973070Z","iopub.execute_input":"2025-02-22T19:04:41.973295Z","iopub.status.idle":"2025-02-22T19:04:44.222030Z","shell.execute_reply.started":"2025-02-22T19:04:41.973276Z","shell.execute_reply":"2025-02-22T19:04:44.221281Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"     0     1     2     3     4     5     6     7\n0  1.0  14.0  13.0   4.0  20.0  33.0  33.0  12.0\n1  2.0  14.0  23.0  10.0  17.0  33.0  33.0  13.0\n2  1.0  14.0   3.0  11.0  18.0  33.0  33.0  16.0\n3  2.0  14.0  14.0  12.0  17.0  33.0  33.0  17.0\n4  1.0  14.0   6.0  16.0  18.0  33.0  33.0  15.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>13.0</td>\n      <td>4.0</td>\n      <td>20.0</td>\n      <td>33.0</td>\n      <td>33.0</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>14.0</td>\n      <td>23.0</td>\n      <td>10.0</td>\n      <td>17.0</td>\n      <td>33.0</td>\n      <td>33.0</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>18.0</td>\n      <td>33.0</td>\n      <td>33.0</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>14.0</td>\n      <td>14.0</td>\n      <td>12.0</td>\n      <td>17.0</td>\n      <td>33.0</td>\n      <td>33.0</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>6.0</td>\n      <td>16.0</td>\n      <td>18.0</td>\n      <td>33.0</td>\n      <td>33.0</td>\n      <td>15.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"y_train_view = pd.DataFrame(y_train)\ny_train_view.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:44.223059Z","iopub.execute_input":"2025-02-22T19:04:44.223395Z","iopub.status.idle":"2025-02-22T19:04:44.231036Z","shell.execute_reply.started":"2025-02-22T19:04:44.223364Z","shell.execute_reply":"2025-02-22T19:04:44.230388Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          0         1         2         3\n0  0.003869  0.027765  0.011614  0.000017\n1  0.000000  0.000000  0.011614  0.000000\n2  0.009596  0.068880  0.028277  0.000014\n3  0.003869  0.027765  0.028277  0.000013\n4  0.014563  0.104635  0.028277  0.000016","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.003869</td>\n      <td>0.027765</td>\n      <td>0.011614</td>\n      <td>0.000017</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.011614</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.009596</td>\n      <td>0.068880</td>\n      <td>0.028277</td>\n      <td>0.000014</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.003869</td>\n      <td>0.027765</td>\n      <td>0.028277</td>\n      <td>0.000013</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.014563</td>\n      <td>0.104635</td>\n      <td>0.028277</td>\n      <td>0.000016</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import torch,math,os\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] =  \"expandable_segments:True\"\nos.environ['CUDA_LAUNCH_BLOCKING']= '1'\nos.environ['TORCH_USE_CUDA_DSA'] = \"1\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:44.231884Z","iopub.execute_input":"2025-02-22T19:04:44.232294Z","iopub.status.idle":"2025-02-22T19:04:47.674497Z","shell.execute_reply.started":"2025-02-22T19:04:44.232247Z","shell.execute_reply":"2025-02-22T19:04:47.673666Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"X_train_tensor = torch.from_numpy(X_train.astype(np.float32) )\ny_train_tensor = torch.from_numpy(y_train.astype(np.float32) )\nX_test_tensor = torch.from_numpy(X_test.astype(np.float32) )\ny_test_tensor = torch.from_numpy(y_test.astype(np.float32) )\nX_eval_tensor = torch.from_numpy(X_eval.astype(np.float32) )\ny_eval_tensor = torch.from_numpy(y_eval.astype(np.float32) )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:47.675285Z","iopub.execute_input":"2025-02-22T19:04:47.675760Z","iopub.status.idle":"2025-02-22T19:04:47.751064Z","shell.execute_reply.started":"2025-02-22T19:04:47.675732Z","shell.execute_reply":"2025-02-22T19:04:47.750285Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import DistilBertModel,DistilBertConfig\nimport torch.nn.init as init\nclass RS_BERT(nn.Module):\n    def __init__(self, vocab_size, hidden_dim,dropout_prob = 0.2):\n        \"\"\"\n        Args:\n            vocab_size (int): Size of your vocabulary.\n            hidden_dim (int): Desired embedding dimension (and transformer hidden size).\n                               For compatibility with DistilBERT, use 768 (or update accordingly).\n        \"\"\"\n        super(RS_BERT, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.dropout_prob = dropout_prob\n        \n        # Load a DistilBert configuration and override the hidden dimension.\n        config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\",dropout=self.dropout_prob,\n                                          attention_dropout=self.dropout_prob)\n        config.dim = hidden_dim  # make sure transformer layers expect our embedding size\n        \n        # Instantiate the DistilBERT model.\n        self.distilbert = DistilBertModel(config)\n        \n        # Freeze all DistilBERT parameters.\n        for param in self.distilbert.parameters():\n            param.requires_grad = False\n        \n        # Zero out the positional embeddings so that no positional information is added.\n        with torch.no_grad():\n            self.distilbert.embeddings.position_embeddings.weight.zero_()\n        \n        # Define the mlp for the four regression heads.\n        self.fc1 = nn.Sequential(nn.Linear(hidden_dim, 50),nn.Tanh(),nn.Dropout(p=self.dropout_prob),nn.Linear(50, 4),\n                                 nn.ReLU(),nn.Dropout(p=self.dropout_prob),nn.Linear(4, 1))\n        self.fc2 = nn.Sequential(nn.Linear(hidden_dim, 50),nn.Tanh(),nn.Dropout(p=self.dropout_prob),nn.Linear(50, 4),\n                                 nn.ReLU(),nn.Dropout(p=self.dropout_prob),nn.Linear(4, 1))\n        self.fc3 = nn.Sequential(nn.Linear(hidden_dim, 50),nn.Tanh(),nn.Dropout(p=self.dropout_prob),nn.Linear(50, 4),\n                                 nn.ReLU(),nn.Dropout(p=self.dropout_prob),nn.Linear(4, 1))\n        self.fc4 = nn.Sequential(nn.Linear(hidden_dim, 50),nn.Tanh(),nn.Dropout(p=self.dropout_prob),nn.Linear(50, 4),\n                                 nn.ReLU(),nn.Dropout(p=self.dropout_prob),nn.Linear(4, 1))\n        self.initialize_final_layer()\n\n    '''i ranked the values of the weight initializations for each head from my observations in demo training runs \n       where i noticed a partially consistent pattern with respect to the magnitude of each    '''\n    def initialize_final_layer(self):\n        fc1_final = self.fc1[-1]\n        init.uniform_(fc1_final.weight, a=0.0, b=0.2)\n        init.uniform_(fc1_final.bias, a=0.0, b=0.1)\n        \n        fc2_final = self.fc2[-1]\n        init.uniform_(fc2_final.weight, a=0.0, b=0.4)\n        init.uniform_(fc2_final.bias, a=0.0, b=0.1)\n        \n        fc3_final = self.fc3[-1]\n        init.uniform_(fc3_final.weight, a=0.0, b=0.7)\n        init.uniform_(fc3_final.bias, a=0.0, b=0.1)\n        \n        fc4_final = self.fc4[-1]\n        init.uniform_(fc4_final.weight, a=0.0, b=0.5)\n        init.uniform_(fc4_final.bias, a=0.0, b=0.1)\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Tensor of token indices with shape (batch_size, seq_length).\n                              (Since the input is tabular data, 'seq_length' may be 1 or more.)\n        Returns:\n            torch.Tensor: Concatenated output of the four regression heads (batch_size, 4).\n        \"\"\"\n        embeds = self.embedding(x.long()) \n        \n        outputs = self.distilbert(inputs_embeds=embeds)\n        # outputs.last_hidden_state has shape: (batch_size, seq_length, hidden_dim)\n        \n        rep = torch.mean(outputs.last_hidden_state,dim=1)  # shape: (batch_size, hidden_dim)\n        \n        out1 = self.fc1(rep)\n        out2 = self.fc2(rep)\n        out3 = self.fc3(rep)\n        out4 = self.fc4(rep)\n        \n        output = torch.cat((out1, out2, out3, out4), dim=1)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:47.751878Z","iopub.execute_input":"2025-02-22T19:04:47.752185Z","iopub.status.idle":"2025-02-22T19:05:04.046974Z","shell.execute_reply.started":"2025-02-22T19:04:47.752156Z","shell.execute_reply":"2025-02-22T19:05:04.046344Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import time, gc, random\n\n# Timing utilities\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer_and_print(local_msg):\n    torch.cuda.synchronize()\n    end_time = time.time()\n    print(\"\\n\" + local_msg)\n    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:04.047741Z","iopub.execute_input":"2025-02-22T19:05:04.048175Z","iopub.status.idle":"2025-02-22T19:05:04.053019Z","shell.execute_reply.started":"2025-02-22T19:05:04.048154Z","shell.execute_reply":"2025-02-22T19:05:04.052203Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_eval(model, train_loader, val_loader, epochs, learning_rate, train_batch_size, eval_batch_size, device=device):\n    model.to(device)\n    \n    # Define optimizer and loss function (modify if necessary)\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n    loss_fn = nn.MSELoss()\n    start_timer()\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n\n        model.train()\n        running_loss = 0.0  # to accumulate loss for printing purposes\n        optimizer.zero_grad()\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            r_int = random.randint(0,train_batch_size-1)\n            inputs, targets = inputs.to(device), targets.to(device)\n            output = model(inputs)\n            loss = loss_fn(output, targets)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            # Accumulate loss for printing\n            running_loss += loss.item()\n\n\n            # Print loss info every 10 batches\n            if (batch_idx + 1) % 10 == 0:\n                # calculate normal crossentropy average loss\n                avg_loss = running_loss / 10\n                cloned_target = targets.detach().clone().cpu().numpy()\n                cloned_output = output.detach().clone().cpu().numpy()\n                \n                mape = (avg_loss/cloned_target.sum()) * 100\n                #cloned_target = npp.reverse_scaling(cloned_target,'y_train')\n                #cloned_output = npp.reverse_scaling(cloned_output,'y_train')\n                \n                print(f\"  [Train] Batch {batch_idx+1}/{len(train_loader)} -  Loss: {avg_loss:.4f}\")\n                print(f'[Train] Batch {batch_idx+1}/{len(train_loader)} calculated Mean Percentage Error {mape}')\n                print(f'output VS target sample comparison, for row{r_int}: {cloned_output[r_int],cloned_target[r_int]}')\n                running_loss = 0.0\n\n        end_timer_and_print('Ending Train Epoch')\n        # --- Evaluation ---\n        model.eval()\n        eval_running_loss = 0.0\n        start_timer()  \n        with torch.no_grad():\n            for batch_idx, (inputs, targets) in enumerate(val_loader):\n                r_int = random.randint(0,eval_batch_size-1)\n                inputs, targets = inputs.to(device), targets.to(device)\n                output = model(inputs)\n                loss = loss_fn(output, targets)\n                eval_running_loss += loss.item()\n\n\n                \n                # Print loss info every 10 batches\n                if (batch_idx + 1) % 10 == 0:\n                    # calculate normal crossentropy average loss\n                    avg_val_loss = eval_running_loss / 10\n                    cloned_target = targets.detach().clone().cpu().numpy()\n                    cloned_output = output.detach().clone().cpu().numpy()\n                \n                    mape = (avg_val_loss/cloned_target.sum()) * 100\n                    cloned_target = npp.reverse_scaling(cloned_target,'y_eval')\n                    cloned_output = npp.reverse_scaling(cloned_output,'y_eval')\n                \n                    print(f\"  [Eval] Batch {batch_idx+1}/{len(val_loader)} -  Loss: {avg_val_loss:.4f}\")\n                    print(f'[Eval] Batch {batch_idx+1}/{len(val_loader)} calculated Mean Percentage Error {mape}')\n                    print(f'Rescaled output VS target sample comparison, for row {r_int}: {cloned_output[r_int],cloned_target[r_int]}')\n                    eval_running_loss = 0.0\n\n\n        \n        end_timer_and_print('Ending Validation Epoch')\n        print('Saving model....')\n        torch.save(model.state_dict(), f'Epoch{epoch+1}_RS_Transformer_weights.pth')     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:04.054036Z","iopub.execute_input":"2025-02-22T19:05:04.054371Z","iopub.status.idle":"2025-02-22T19:05:04.080119Z","shell.execute_reply.started":"2025-02-22T19:05:04.054316Z","shell.execute_reply":"2025-02-22T19:05:04.079503Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"vocab_size = 34\nhidden_dim= 540\ndropout = 0.4\nmodel = RS_BERT(vocab_size, hidden_dim,dropout)\n\n\ntrain_batch_size = 10000\neval_batch_size = 10000\nlearning_rate = 0.001\nepochs = 1\n\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=train_batch_size,shuffle=True,drop_last=True)\nval_dataset = TensorDataset(X_eval_tensor, y_eval_tensor)\nval_loader = DataLoader(val_dataset, batch_size=eval_batch_size,drop_last=True)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:04.081021Z","iopub.execute_input":"2025-02-22T19:05:04.081288Z","iopub.status.idle":"2025-02-22T19:05:05.921972Z","shell.execute_reply.started":"2025-02-22T19:05:04.081240Z","shell.execute_reply":"2025-02-22T19:05:05.921085Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6e022454c25455eb2279b1753346b3f"}},"metadata":{}},{"name":"stdout","text":"RS_BERT(\n  (embedding): Embedding(34, 540)\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 540, padding_idx=0)\n      (position_embeddings): Embedding(512, 540)\n      (LayerNorm): LayerNorm((540,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.4, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): DistilBertSdpaAttention(\n            (dropout): Dropout(p=0.4, inplace=False)\n            (q_lin): Linear(in_features=540, out_features=540, bias=True)\n            (k_lin): Linear(in_features=540, out_features=540, bias=True)\n            (v_lin): Linear(in_features=540, out_features=540, bias=True)\n            (out_lin): Linear(in_features=540, out_features=540, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((540,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.4, inplace=False)\n            (lin1): Linear(in_features=540, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=540, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((540,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (fc1): Sequential(\n    (0): Linear(in_features=540, out_features=50, bias=True)\n    (1): Tanh()\n    (2): Dropout(p=0.4, inplace=False)\n    (3): Linear(in_features=50, out_features=4, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.4, inplace=False)\n    (6): Linear(in_features=4, out_features=1, bias=True)\n  )\n  (fc2): Sequential(\n    (0): Linear(in_features=540, out_features=50, bias=True)\n    (1): Tanh()\n    (2): Dropout(p=0.4, inplace=False)\n    (3): Linear(in_features=50, out_features=4, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.4, inplace=False)\n    (6): Linear(in_features=4, out_features=1, bias=True)\n  )\n  (fc3): Sequential(\n    (0): Linear(in_features=540, out_features=50, bias=True)\n    (1): Tanh()\n    (2): Dropout(p=0.4, inplace=False)\n    (3): Linear(in_features=50, out_features=4, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.4, inplace=False)\n    (6): Linear(in_features=4, out_features=1, bias=True)\n  )\n  (fc4): Sequential(\n    (0): Linear(in_features=540, out_features=50, bias=True)\n    (1): Tanh()\n    (2): Dropout(p=0.4, inplace=False)\n    (3): Linear(in_features=50, out_features=4, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.4, inplace=False)\n    (6): Linear(in_features=4, out_features=1, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"train_eval(model, train_loader, val_loader, epochs, learning_rate,train_batch_size,eval_batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:05.922815Z","iopub.execute_input":"2025-02-22T19:05:05.923145Z","iopub.status.idle":"2025-02-22T19:21:55.427081Z","shell.execute_reply.started":"2025-02-22T19:05:05.923120Z","shell.execute_reply":"2025-02-22T19:21:55.426407Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/1\n  [Train] Batch 10/199 -  Loss: 0.0657\n[Train] Batch 10/199 calculated Mean Percentage Error 0.0006315029553330013\noutput VS target sample comparison, for row6064: (array([0.0390901 , 0.2228476 , 0.0641335 , 0.32512942], dtype=float32), array([0.29274693, 0.35117233, 0.4248891 , 0.1597878 ], dtype=float32))\n  [Train] Batch 20/199 -  Loss: 0.0651\n[Train] Batch 20/199 calculated Mean Percentage Error 0.0006291322643514\noutput VS target sample comparison, for row4870: (array([0.03916832, 0.322973  , 0.41087005, 0.06576046], dtype=float32), array([0.02602352, 0.18675303, 0.08632252, 0.01551976], dtype=float32))\n  [Train] Batch 30/199 -  Loss: 0.0646\n[Train] Batch 30/199 calculated Mean Percentage Error 0.0006234168095405292\noutput VS target sample comparison, for row3831: (array([0.147547  , 0.06916529, 0.24701232, 0.17280254], dtype=float32), array([0.01357027, 0.26507515, 0.5789075 , 0.18558685], dtype=float32))\n  [Train] Batch 40/199 -  Loss: 0.0636\n[Train] Batch 40/199 calculated Mean Percentage Error 0.000614519528684092\noutput VS target sample comparison, for row1962: (array([0.10264082, 0.25071102, 0.12860693, 0.27365115], dtype=float32), array([3.3848614e-02, 2.4745677e-01, 2.8201761e-02, 7.2895811e-05],\n      dtype=float32))\n  [Train] Batch 50/199 -  Loss: 0.0633\n[Train] Batch 50/199 calculated Mean Percentage Error 0.0006117981406633519\noutput VS target sample comparison, for row5556: (array([0.03940224, 0.18029356, 0.05595306, 0.13610321], dtype=float32), array([0.17741637, 0.6260477 , 0.13500942, 0.08259211], dtype=float32))\n  [Train] Batch 60/199 -  Loss: 0.0628\n[Train] Batch 60/199 calculated Mean Percentage Error 0.0006035844539928338\noutput VS target sample comparison, for row1869: (array([0.0623449 , 0.04535034, 0.23796952, 0.16229725], dtype=float32), array([0.07397088, 0.5440882 , 0.18810752, 0.1223701 ], dtype=float32))\n  [Train] Batch 70/199 -  Loss: 0.0621\n[Train] Batch 70/199 calculated Mean Percentage Error 0.0006006972074599397\noutput VS target sample comparison, for row1190: (array([0.03955939, 0.07398965, 0.27920067, 0.08414478], dtype=float32), array([0.       , 0.       , 0.0237439, 0.       ], dtype=float32))\n  [Train] Batch 80/199 -  Loss: 0.0618\n[Train] Batch 80/199 calculated Mean Percentage Error 0.0005952361501052798\noutput VS target sample comparison, for row2647: (array([0.05141327, 0.02257514, 0.24221   , 0.21071273], dtype=float32), array([0.0352355 , 0.2514684 , 0.09721021, 0.00899702], dtype=float32))\n  [Train] Batch 90/199 -  Loss: 0.0610\n[Train] Batch 90/199 calculated Mean Percentage Error 0.0005896590747468348\noutput VS target sample comparison, for row606: (array([0.05682087, 0.32989898, 0.24007264, 0.1416316 ], dtype=float32), array([0.02917353, 0.21778524, 0.42873895, 0.14576147], dtype=float32))\n  [Train] Batch 100/199 -  Loss: 0.0610\n[Train] Batch 100/199 calculated Mean Percentage Error 0.0005817618528361927\noutput VS target sample comparison, for row9426: (array([0.0397873 , 0.2125313 , 0.05739215, 0.22532427], dtype=float32), array([0.03810842, 0.27351123, 0.12594207, 0.00182352], dtype=float32))\n  [Train] Batch 110/199 -  Loss: 0.0601\n[Train] Batch 110/199 calculated Mean Percentage Error 0.0005808052293884013\noutput VS target sample comparison, for row7606: (array([0.03986605, 0.07570124, 0.409924  , 0.06853391], dtype=float32), array([0.02180572, 0.09207086, 0.44144425, 0.10362577], dtype=float32))\n  [Train] Batch 120/199 -  Loss: 0.0600\n[Train] Batch 120/199 calculated Mean Percentage Error 0.0005747240783250952\noutput VS target sample comparison, for row8903: (array([0.03994242, 0.2082309 , 0.06521557, 0.08782328], dtype=float32), array([0.03447475, 0.20382586, 0.5833653 , 0.16752665], dtype=float32))\n  [Train] Batch 130/199 -  Loss: 0.0598\n[Train] Batch 130/199 calculated Mean Percentage Error 0.0005772597898191504\noutput VS target sample comparison, for row4744: (array([0.040018  , 0.04827307, 0.27941373, 0.08943649], dtype=float32), array([0.02391721, 0.42395872, 0.6832306 , 0.3146982 ], dtype=float32))\n  [Train] Batch 140/199 -  Loss: 0.0593\n[Train] Batch 140/199 calculated Mean Percentage Error 0.0005733451448411435\noutput VS target sample comparison, for row7972: (array([0.05781207, 0.22779907, 0.11801611, 0.09850011], dtype=float32), array([0.01637378, 0.30682826, 0.67610484, 0.29570788], dtype=float32))\n  [Train] Batch 150/199 -  Loss: 0.0589\n[Train] Batch 150/199 calculated Mean Percentage Error 0.0005648812449697508\noutput VS target sample comparison, for row8576: (array([0.0505183 , 0.23996672, 0.1223021 , 0.08316613], dtype=float32), array([0.04635055, 0.33828825, 0.04798953, 0.00275089], dtype=float32))\n  [Train] Batch 160/199 -  Loss: 0.0584\n[Train] Batch 160/199 calculated Mean Percentage Error 0.0005650250572341405\noutput VS target sample comparison, for row8100: (array([0.09407066, 0.3941877 , 0.06765397, 0.07001837], dtype=float32), array([0.02753774, 0.4259832 , 0.6324882 , 0.45454115], dtype=float32))\n  [Train] Batch 170/199 -  Loss: 0.0583\n[Train] Batch 170/199 calculated Mean Percentage Error 0.0005641801742910439\noutput VS target sample comparison, for row8851: (array([0.0925134 , 0.3409573 , 0.09887284, 0.2852193 ], dtype=float32), array([0.23091207, 0.51906365, 0.37009925, 0.20528305], dtype=float32))\n  [Train] Batch 180/199 -  Loss: 0.0579\n[Train] Batch 180/199 calculated Mean Percentage Error 0.000566588694780699\noutput VS target sample comparison, for row1001: (array([0.05368399, 0.03107505, 0.11231545, 0.09282453], dtype=float32), array([0.02023751, 0.3194753 , 0.7905294 , 0.47566342], dtype=float32))\n  [Train] Batch 190/199 -  Loss: 0.0579\n[Train] Batch 190/199 calculated Mean Percentage Error 0.0005568267252845181\noutput VS target sample comparison, for row6569: (array([0.05749591, 0.17480743, 0.6275053 , 0.09947744], dtype=float32), array([0.0116219 , 0.11139638, 0.38592383, 0.10272007], dtype=float32))\n\nEnding Train Epoch\nTotal execution time = 938.146 sec\nMax memory used by tensors = 14811288576 bytes\n  [Eval] Batch 10/29 -  Loss: 0.0475\n[Eval] Batch 10/29 calculated Mean Percentage Error 0.0004416489678964984\nRescaled output VS target sample comparison, for row 8658: (array([14940.08705129,  6360.98955424, 15139.37470794,  7829.40262662]), array([25389.7639082 ,  6368.81416568, 26482.65792305, 14378.44404266]))\n  [Eval] Batch 20/29 -  Loss: 0.0467\n[Eval] Batch 20/29 calculated Mean Percentage Error 0.00044373559635625415\nRescaled output VS target sample comparison, for row 673: (array([12298.55515563,  4036.6110513 , 13277.57563972,  6327.10838656]), array([ 3845.11548975,  7032.35264619, 34100.0789602 , 20461.16708629]))\n\nEnding Validation Epoch\nTotal execution time = 69.882 sec\nMax memory used by tensors = 2850523648 bytes\nSaving model....\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"test_batch_size = 10000\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntest_loader = DataLoader(test_dataset, batch_size=test_batch_size,drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:21:55.427909Z","iopub.execute_input":"2025-02-22T19:21:55.428212Z","iopub.status.idle":"2025-02-22T19:21:55.432245Z","shell.execute_reply.started":"2025-02-22T19:21:55.428184Z","shell.execute_reply":"2025-02-22T19:21:55.431478Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"with torch.no_grad():\n    loss_fn = nn.MSELoss()\n    test_running_loss = 0.0\n    for batch_idx, (inputs, targets) in enumerate(test_loader):\n        r_int = random.randint(0,test_batch_size-1)\n        inputs, targets = inputs.to(device), targets.to(device)\n        output = model(inputs)\n        loss = loss_fn(output, targets)\n        test_running_loss += loss.item()\n\n                \n        # Print loss info every 10 batches\n        if (batch_idx + 1) % 10 == 0:\n            # calculate normal crossentropy average loss\n            avg_test_loss = test_running_loss / 10\n            cloned_target = targets.detach().clone().cpu().numpy()\n            cloned_output = output.detach().clone().cpu().numpy()\n            mape = (avg_test_loss/cloned_target.sum()) * 100\n            cloned_target = npp.reverse_scaling(cloned_target,'y_test')\n            cloned_output = npp.reverse_scaling(cloned_output,'y_test')\n                \n            print(f\"[Test] Batch {batch_idx+1}/{len(test_loader)} -  Loss: {avg_test_loss:.4f}\")\n            print(f'[Test] Batch {batch_idx+1}/{len(test_loader)} calculated Mean Percentage Error {mape}')\n            print(f'Rescaled output VS target sample comparison, for row {r_int}: {cloned_output[r_int],cloned_target[r_int]}')\n            test_running_loss = 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:21:55.433084Z","iopub.execute_input":"2025-02-22T19:21:55.433384Z","iopub.status.idle":"2025-02-22T19:24:44.318962Z","shell.execute_reply.started":"2025-02-22T19:21:55.433355Z","shell.execute_reply":"2025-02-22T19:24:44.318283Z"}},"outputs":[{"name":"stdout","text":"[Test] Batch 10/69 -  Loss: 0.0446\n[Test] Batch 10/69 calculated Mean Percentage Error 0.00042697657437991437\nRescaled output VS target sample comparison, for row 4056: (array([11853.6965412 ,  3712.77151866, 14670.55861211,  8494.96577107]), array([ 5580.68965339,  8796.16127061, 33835.02329798, 19933.77051519]))\n[Test] Batch 20/69 -  Loss: 0.0448\n[Test] Batch 20/69 calculated Mean Percentage Error 0.00042659670759097833\nRescaled output VS target sample comparison, for row 7018: (array([ 9704.2848307 ,  6781.89929376, 18963.08850739,  8813.1464231 ]), array([12858.2529354 ,  9316.71492544, 19211.29611288, 15600.35499449]))\n[Test] Batch 30/69 -  Loss: 0.0447\n[Test] Batch 30/69 calculated Mean Percentage Error 0.00042770513582314306\nRescaled output VS target sample comparison, for row 9239: (array([ 9730.54129804,  3909.59825994, 14600.59463513,  8625.52936731]), array([66687.16116911, 10077.29857094, 22185.7622873 , 13024.09810566]))\n[Test] Batch 40/69 -  Loss: 0.0448\n[Test] Batch 40/69 calculated Mean Percentage Error 0.0004213407108003479\nRescaled output VS target sample comparison, for row 2859: (array([16515.35569385,  3576.83755916, 17324.67301855,  8233.30909769]), array([ 2080.29816003,  3735.52556179, 31242.24940968, 10106.89992511]))\n[Test] Batch 50/69 -  Loss: 0.0445\n[Test] Batch 50/69 calculated Mean Percentage Error 0.0004236010719549242\nRescaled output VS target sample comparison, for row 8409: (array([14202.10726955,  4522.861531  ,  9964.23732498,  9181.81562145]), array([3674.49686822, 2643.52463252, 5512.19037867,  708.39012598]))\n[Test] Batch 60/69 -  Loss: 0.0444\n[Test] Batch 60/69 calculated Mean Percentage Error 0.0004250074581615791\nRescaled output VS target sample comparison, for row 206: (array([13370.50754002,  4345.73582212, 13359.71469283,  9799.08204761]), array([6241.54769237, 4507.50155487,    0.        ,  228.7181105 ]))\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}