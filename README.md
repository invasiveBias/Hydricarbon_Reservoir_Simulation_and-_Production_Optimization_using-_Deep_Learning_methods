# Hydricarbon_Reservoir_Simulation_and-_Production_Optimization_using-_Deep_Learning_methods
This project is split in 2 parts. Part 1 is the simulation of an Hydrocarbon reservoir by finetuning a pretrained deep learning model to map Geologic, Geometric and 2 Production features of Hydrocarbon wells to Gas, Oil and Water production + injection rates from each well and part 2 is the training of a Deep Reinforcement Learning agent to optimize the afroementioned production rates from the trained Reservoir Simulation model.
The Distil-BERT variant of the BERT family of models was the selected pretrained model for the Reservoir simulation task, it was chosen because of it's state of the art efficiency for feature representation while being sized at a range that's reasonable for low resource training (On the T-4 GPU from kaggle) and inferencing. The Proximal Policy Optimization (PPO) Algorithm was selected as the DRL algorithm to be trained for production optimization, PPO being a policy gradient algorithm, was selected for it's efficiency at approximating policies in a continious action space while making sure policy updates are not too large to destabilize training.
For Finetuning, the pretrained weights of the Distil-BERT model were frozen and it's output layer was replaced with a Multi-head Regressor which has 4 output heads for predicting Gas, Oil and Water production rates along with Water injection rate. Only the weights of the multi-head regressor was initialized during training, so it can learn how to predict the Prod+Inj values from the representations outputted by the frozen Distil-BERT layers. The DRL training environment was built for the PPO agent to predict continious pressure injection values from observing Geologic, Geometric and a single Production feature, the output of the PPO agent becomes the 2nd production feature which along with the observation parameters is sent to the already trained reservoir simulator to predicts Oil, Gas and water Prod+Inj rates. The predicted rates are used along with parameters like Oil Price, Gas price and production costs to estimate the Economic value of running the well by calculating total Revenue, total Cost and Gross Present value (GPV) per production rate. The GPV estimation is parameterised as the reward function for the environment, meaning that the DRL agent is expected to output the optimal pressure injection values for maximizing Gross Production Values at every timestep.
Both the Reservoir Simulator and the GPV optimzer agent were trained on a personally preprocessed variant of the Oil Resrvoir Simulation Dataset (ORSD). For the Reservoir simulator the dataset contained 8 input features: porosuity, horizontal_permeability, vertical_permeability (Geologic), x, y,z (Geometric), well_type, injected_pressure (Production) and 4 output features : Gas production,Oil production, water injection ,water production. For the reservoir simulator it was given batches of the data with the 8 input features and was expected to simultaeneously predict the 4 output features under a Mean Square Error Loss while for the GPV optimizer, it was given all but the injected_pressure as state observation, with the intention for the agent to predict optimal injected_pressure values under the GPV reward estimation and PPO loss. Notably, in order to model a larger production field instead of a single, the nevironment was vecotrized for the agent to be able to simulataneously train on 25 wells. The original dataset was gotten from the IBM data Asset exchange platform, to read more on it check here: https://dax-cdn.cdn.appdomain.cloud/dax-oil-reservoir-simulations/1.0.0/data-preview/Part%201%20-%20Dataset%20overview.html
